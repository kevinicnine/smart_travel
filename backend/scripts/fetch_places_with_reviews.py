"""
Fetch Google Places details (with reviews) for existing places in db.json,
to build training text for classification. Includes a simple quota guard
so it stops near your daily free quota.

Usage:
  GOOGLE_MAPS_API_KEY=your_key python3 backend/scripts/fetch_places_with_reviews.py

Outputs:
  backend/data/places_with_reviews.json  (list of dicts with name/address/geometry/reviews/category/tags)

Notes:
  - Uses Text Search to find place_id by name+city, then Place Details to fetch fields:
    name, formatted_address, geometry, editorial_summary, reviews, rating, user_ratings_total, types
  - Extracts tags from name/summary/reviews/types with keyword rules (multi-label).
  - Uses a local text classifier to filter low-signal reviews when enough data is available.
  - MAX_REQUESTS limits total HTTP calls (search + details eachÁÆó‰∏ÄÊ¨°) to avoidË∂ÖÈ°ç„ÄÇ
  - ÈÅáÂà∞ OVER_QUERY_LIMIT/429 ÊúÉÁ´ãÂç≥ÂÅúÊ≠¢„ÄÇ
"""
from __future__ import annotations

import json
import os
import ssl
import sys
import time
import urllib.parse
import urllib.request
from pathlib import Path
from typing import Dict, Any, List

ROOT = Path(__file__).resolve().parents[1]
DB_PATH = ROOT / "data" / "db.json"
OUT_PATH = ROOT / "data" / "places_with_reviews.json"
API_KEY = os.environ.get("GOOGLE_MAPS_API_KEY")

MAX_REQUESTS = 300  # Á∏ΩË´ãÊ±ÇÊï∏‰∏äÈôêÔºàsearch+details ÈÉΩÁÆóÔºâ
SLEEP_BETWEEN = 0.1  # ÁßíÔºåÈÅøÂÖçÈÅéÂø´
REVIEWS_LIMIT = 5  # Âè™‰øùÁïôÊúÄÁõ∏ÈóúÁöÑÂâçÂπæÂâáË©ïË´ñ
MIN_REVIEW_LEN = 12  # Ë©ïË´ñÊúÄÂ∞ëÂ≠óÊï∏
MIN_MODEL_PROB = 0.55  # Ê®°ÂûãÂà§Êñ∑ÁÇ∫„ÄåÊúâÊïàË©ïË´ñ„ÄçÁöÑÈñÄÊ™ª
USE_LOCAL_MODEL = True
MERGE_TO_DB = True  # ÊäìÂÆåÂæåÁõ¥Êé•Âêà‰Ωµ tags Âõû db.json

NOISE_HINTS = [
    "ÂìàÂìà",
    "ÂëµÂëµ",
    "üëç",
    "ËÆö",
    "Êé®",
    "Êé®Êé®",
    "Â•ΩËÆö",
    "Ë∂ÖËÆö",
    "ÂæàÊ£í",
    "‰∏çÈåØ",
]
INFO_HINTS = [
    "‰∫§ÈÄö",
    "ÂÅúËªä",
    "Â∞éË¶Ω",
    "Â±ïË¶Ω",
    "ÈñÄÁ•®",
    "Á•®ÂÉπ",
    "Áí∞Â¢É",
    "ÊúçÂãô",
    "Ë¶™Â≠ê",
    "Ê≠•ÈÅì",
    "ÊôØËâ≤",
    "È¢®ÊôØ",
    "È§êÂª≥",
    "ÂíñÂï°",
    "‰∫∫ÊΩÆ",
    "ÊéíÈöä",
    "ÂªÅÊâÄ",
    "Êé®Ëñ¶",
    "Âú∞ÂùÄ",
    "Êç∑ÈÅã",
    "ÂÖ¨Ëªä",
]
FIELDS = ",".join(
    [
      "place_id",
      "name",
      "formatted_address",
      "geometry",
      "editorial_summary",
      "reviews",
      "rating",
      "user_ratings_total",
      "types",
    ]
)

if not API_KEY:
    sys.exit("Ë´ãÂÖàÂú®Áí∞Â¢ÉËÆäÊï∏Ë®≠ÂÆö GOOGLE_MAPS_API_KEY")


# tag keywords (kw -> tag)
INTEREST_KEYWORDS = [
    ("ËßÄÂÖâÂ∑•Âª†", "creative_park"),
    ("Â∑•Âª†", "creative_park"),
    ("ÈÖíÂª†", "creative_park"),
    ("ÊñáÂâµ", "creative_park"),
    ("ÂúíÂçÄ", "creative_park"),
    ("Â§úÂ∏Ç", "night_market"),
    ("ÂïÜÂúà", "department_store"),
    ("Ê∞¥ÊóèÈ§®", "aquarium"),
    ("Êµ∑ÁîüÈ§®", "aquarium"),
    ("ÂçöÁâ©È§®", "museum"),
    ("ÁæéË°ìÈ§®", "museum"),
    ("ÊñáÂåñÈ§®", "museum"),
    ("Â±ïË¶ΩÈ§®", "museum"),
    ("Èü≥Ê®ÇÂª≥", "concert_hall"),
    ("ÊºîËóù", "concert_hall"),
    ("ËóùÊñá‰∏≠ÂøÉ", "concert_hall"),
    ("ÈõªÂΩ±Èô¢", "cinema"),
    ("ÂΩ±Âüé", "cinema"),
    ("ÈÅäÊ®ÇÂúí", "amusement"),
    ("‰∏ªÈ°åÊ®ÇÂúí", "amusement"),
    ("ÂãïÁâ©Âúí", "zoo"),
    ("ÈáéÁîüÂãïÁâ©", "zoo"),
    ("ÂíñÂï°", "cafe"),
    ("È§êÂª≥", "restaurant"),
    ("ÁæéÈ£ü", "restaurant"),
    ("È§êÈ£≤", "restaurant"),
    ("Â∞èÂêÉ", "street_food"),
    ("Ë∑ØÈÇäÊî§", "street_food"),
    ("Â∞èÂêÉË°ó", "street_food"),
    ("ÁôæË≤®", "department_store"),
    ("ÂïÜÂ†¥", "department_store"),
    ("Ë≥ºÁâ©", "department_store"),
    ("Êâã‰Ωú", "handcraft_shop"),
    ("Â∑•Ëóù", "handcraft_shop"),
    ("Èô∂Ëóù", "handcraft_shop"),
    ("Ëæ≤Â†¥", "farm"),
    ("ÁâßÂ†¥", "farm"),
    ("‰ºëÈñíËæ≤Â†¥", "farm"),
    ("Èú≤Ááü", "camping"),
    ("ÈáéÁáü", "camping"),
    ("Ëá™Ë°åËªä", "bike"),
    ("ËÖ≥Ë∏èËªä", "bike"),
    ("ÂñÆËªä", "bike"),
    ("Ê∞¥‰∏äÊ¥ªÂãï", "water_sport"),
    ("ÊΩõÊ∞¥", "water_sport"),
    ("Êà≤Ê∞¥", "water_sport"),
    ("Ë°ùÊµ™", "water_sport"),
    ("ÂàíËàπ", "water_sport"),
    ("ÁêÉÂ†¥", "ball_sport"),
    ("ÁêÉÈ°û", "ball_sport"),
    ("Ê∫´Ê≥â", "hot_spring"),
    ("ÊπØÂ±ã", "hot_spring"),
    ("ÁÄëÂ∏É", "waterfall"),
    ("Êµ∑ÁÅò", "beach"),
    ("Ê≤ôÁÅò", "beach"),
    ("Êµ∑Ê∞¥Êµ¥Â†¥", "beach"),
    ("Êµ∑Â≤∏", "beach"),
    ("Êπñ", "lake_river"),
    ("Ê≤≥", "lake_river"),
    ("Ê∫™", "lake_river"),
    ("ÊΩüÊπñ", "lake_river"),
    ("Ê∞¥Â∫´", "lake_river"),
    ("Âè§Âéù", "heritage"),
    ("Âè§Ëπü", "heritage"),
    ("ËÄÅË°ó", "heritage"),
    ("Ê≠∑Âè≤", "heritage"),
    ("ÊñáÂåñ", "heritage"),
    ("Á†≤Âè∞", "heritage"),
    ("ÂüéÂ†°", "heritage"),
    ("ÂüéÈñÄ", "heritage"),
    ("ÂüéÁâÜ", "heritage"),
    ("ÊïÖÂ±Ö", "heritage"),
    ("Á¥ÄÂøµÈ§®", "heritage"),
    ("Ëá™ÁÑ∂", "national_park"),
    ("ÁîüÊÖã", "national_park"),
    ("Â±±", "national_park"),
    ("Ê≠•ÈÅì", "national_park"),
    ("Ê£ÆÊûó", "national_park"),
    ("Ê£ÆÊûóÈÅäÊ®ÇÂçÄ", "national_park"),
    ("È¢®ÊôØÂçÄ", "national_park"),
    ("ÊøïÂú∞", "national_park"),
    ("ÂÆóÊïô", "temple"),
    ("Âªü", "temple"),
    ("ÂØ∫", "temple"),
    ("ÂÆÆ", "temple"),
]

PLACE_TYPE_TAGS = {
    "museum": "museum",
    "art_gallery": "museum",
    "amusement_park": "amusement",
    "aquarium": "aquarium",
    "zoo": "zoo",
    "park": "national_park",
    "campground": "camping",
    "shopping_mall": "department_store",
    "restaurant": "restaurant",
    "cafe": "cafe",
    "tourist_attraction": "heritage",
    "place_of_worship": "temple",
}


def extract_tags(text: str, types: list[str], fallback: str) -> list[str]:
    tags = set()
    if fallback:
        tags.add(fallback)
    for kw, tag in INTEREST_KEYWORDS:
        if kw in text:
            tags.add(tag)
    for t in types:
        mapped = PLACE_TYPE_TAGS.get(t)
        if mapped:
            tags.add(mapped)
    if not tags:
        tags.add("other")
    return sorted(tags)


def _label_review(text: str) -> int | None:
    if len(text) < MIN_REVIEW_LEN:
        return 0
    if any(term in text for term in NOISE_HINTS) and len(text) < 30:
        return 0
    if any(term in text for term in INFO_HINTS):
        return 1
    return None


def _train_review_model(texts: list[str]):
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.linear_model import LogisticRegression
        from sklearn.pipeline import make_pipeline
    except Exception as exc:
        print("sklearn not available, skip model filtering:", exc)
        return None

    labels = []
    labeled_texts = []
    for t in texts:
        label = _label_review(t)
        if label is None:
            continue
        labeled_texts.append(t)
        labels.append(label)

    if len(set(labels)) < 2 or len(labels) < 80:
        print("Not enough labeled reviews for model; skip model filtering.")
        return None

    model = make_pipeline(
        TfidfVectorizer(max_features=8000),
        LogisticRegression(max_iter=1000),
    )
    model.fit(labeled_texts, labels)
    print("Trained local review filter on", len(labels), "samples.")
    return model


def clean_reviews(reviews: list[str], model=None) -> list[str]:
    seen = set()
    cleaned: list[str] = []
    for raw in reviews:
        text = (raw or "").strip()
        if len(text) < MIN_REVIEW_LEN:
            continue
        if text in seen:
            continue
        if model is not None:
            try:
                score = model.predict_proba([text])[0][1]
                if score < MIN_MODEL_PROB:
                    continue
            except Exception:
                pass
        seen.add(text)
        cleaned.append(text)
    return cleaned

def fetch_json(url: str, params: Dict[str, Any]) -> Dict[str, Any]:
    global request_count
    if request_count >= MAX_REQUESTS:
        raise RuntimeError("Â∑≤ÈÅî MAX_REQUESTSÔºåÂÅúÊ≠¢‰ª•ÈÅøÂÖçË∂ÖÈ°ç")
    params["key"] = API_KEY
    qs = urllib.parse.urlencode(params, safe=",")
    full_url = f"{url}?{qs}"
    req = urllib.request.Request(full_url, headers={"User-Agent": "smart-travel/1.0"})
    with urllib.request.urlopen(req, timeout=20, context=_ssl_context()) as resp:
        data = json.loads(resp.read().decode("utf-8"))
    request_count += 1
    if data.get("status") in {"OVER_QUERY_LIMIT", "RESOURCE_EXHAUSTED"}:
        raise RuntimeError("OVER_QUERY_LIMIT/RESOURCE_EXHAUSTEDÔºåÂ∑≤ÂÅúÊ≠¢")
    return data


def _ssl_context() -> ssl.SSLContext:
    try:
        import certifi

        return ssl.create_default_context(cafile=certifi.where())
    except Exception:
        return ssl.create_default_context()


def text_search(query: str) -> str | None:
    data = fetch_json(
        "https://maps.googleapis.com/maps/api/place/textsearch/json",
        {"query": query, "language": "zh-TW"},
    )
    results = data.get("results") or []
    if not results:
        return None
    return results[0].get("place_id")


def place_details(place_id: str) -> Dict[str, Any] | None:
    data = fetch_json(
        "https://maps.googleapis.com/maps/api/place/details/json",
        {
            "place_id": place_id,
            "language": "zh-TW",
            "reviews_sort": "most_relevant",
            "fields": FIELDS,
        },
    )
    if data.get("status") != "OK":
        return None
    return data.get("result") or {}


def main():
    global request_count
    request_count = 0

    if not DB_PATH.exists():
        sys.exit(f"Êâæ‰∏çÂà∞ {DB_PATH}")
    db = json.loads(DB_PATH.read_text(encoding="utf-8"))
    places = db.get("places") or []
    output: List[Dict[str, Any]] = []
    review_pool: List[str] = []
    staged: List[Dict[str, Any]] = []

    for idx, p in enumerate(places):
        name = p.get("name") or ""
        city = p.get("city") or ""
        category = p.get("category") or "other"
        tags = p.get("tags")
        if not isinstance(tags, list):
            tags = [category]
        if not name:
            continue

        query = f"{name} {city}".strip()
        try:
            pid = text_search(query)
        except RuntimeError as e:
            print(f"[STOP] {e}")
            break
        except Exception as e:
            print(f"[WARN] searchÂ§±Êïó: {name} ({e})")
            continue

        if not pid:
            print(f"[SKIP] Êâæ‰∏çÂà∞ place_id: {name}")
            continue

        time.sleep(SLEEP_BETWEEN)
        try:
            detail = place_details(pid)
        except RuntimeError as e:
            print(f"[STOP] {e}")
            break
        except Exception as e:
            print(f"[WARN] detailsÂ§±Êïó: {name} ({e})")
            continue

        if not detail:
            print(f"[SKIP] details ÁÑ°Ë≥áÊñô: {name}")
            continue

        reviews = detail.get("reviews") or []
        raw_reviews = [r.get("text", "") for r in reviews]
        review_pool.extend(raw_reviews)
        staged.append(
            {
                "source_name": name,
                "category": category,
                "place_id": pid,
                "name": detail.get("name", ""),
                "address": detail.get("formatted_address", ""),
                "lat": detail.get("geometry", {}).get("location", {}).get("lat"),
                "lng": detail.get("geometry", {}).get("location", {}).get("lng"),
                "rating": detail.get("rating"),
                "user_ratings_total": detail.get("user_ratings_total"),
                "types": detail.get("types") or [],
                "editorial_summary": (detail.get("editorial_summary") or {}).get("overview", ""),
                "raw_reviews": raw_reviews,
            }
        )

        if (idx + 1) % 20 == 0:
            print(f"ÈÄ≤Â∫¶ {idx+1}/{len(places)}, Áî®Èáè {request_count}/{MAX_REQUESTS}")
        time.sleep(SLEEP_BETWEEN)

        if request_count >= MAX_REQUESTS:
            print("Â∑≤ÈÅî MAX_REQUESTSÔºåÊèêÂâçÁµêÊùü„ÄÇ")
            break

    model = _train_review_model(review_pool) if USE_LOCAL_MODEL else None
    for item in staged:
        reviews_texts = clean_reviews(item.pop("raw_reviews"), model)[:REVIEWS_LIMIT]
        editorial = item.get("editorial_summary") or ""
        types = item.get("types") or []
        text_blob = " ".join([item.get("source_name", ""), editorial, " ".join(reviews_texts)])
        tags = extract_tags(text_blob, types, item.get("category") or "other")
        item["tags"] = tags
        item["reviews"] = reviews_texts
        output.append(item)

    OUT_PATH.write_text(json.dumps(output, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"ÂÆåÊàêÔºåÂØ´ÂÖ• {OUT_PATH}, Á∏ΩÁ≠ÜÊï∏ {len(output)}, API Ë´ãÊ±Ç {request_count}")
    if MERGE_TO_DB:
        merge_into_db(output)


def merge_into_db(reviews: list[Dict[str, Any]]) -> None:
    db = json.loads(DB_PATH.read_text(encoding="utf-8"))
    places = db.get("places") or []
    tag_map: dict[str, list[str]] = {}
    for item in reviews:
        name = (item.get("source_name") or item.get("name") or "").strip()
        tags = item.get("tags")
        if not name or not isinstance(tags, list) or not tags:
            continue
        tag_map[name] = [t for t in tags if isinstance(t, str) and t.strip()]

    updated = 0
    for p in places:
        name = (p.get("name") or "").strip()
        if not name or name not in tag_map:
            continue
        tags = tag_map[name]
        if not tags:
            continue
        p["tags"] = tags
        p["category"] = tags[0]
        updated += 1

    DB_PATH.write_text(json.dumps(db, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"Â∑≤Âêà‰Ωµ tags Âõû db.json: {updated} Á≠Ü")


if __name__ == "__main__":
    main()
